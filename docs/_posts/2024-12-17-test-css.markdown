---
layout: post
title: "Understanding LLM Responses: A Deep Dive"
date: 2024-12-17
categories: ai llm
---

<style>
/* Make the page wider */
.page-content .wrapper,
.wrapper {
    max-width: 1400px !important;
    margin: 0 auto;
}

.inner {
    max-width: 1400px !important;
}

/* Two-column layout styles */
.container {
    display: flex;
    gap: 1rem;
    margin: 1rem 0;
}

.text-column {
    flex: 1;
}

.prompt-column {
    flex: 1;
    background: #f5f5f5;
    padding: 0.5rem;
    border-radius: 8px;
}

.prompt {
    font-family: monospace;
    margin-bottom: 0.5rem;
}

/* Style improvements for prompts */
.prompt-label {
    color: #666;
    font-size: 0.9em;
    font-weight: bold;
    margin-bottom: 0.3rem;
}

.response {
    border-top: 1px solid #ddd;
    margin-top: 0.5rem;
    padding-top: 0.5rem;
}

/* Control list spacing in responses */
.response ul {
    margin-top: 0.3rem;
    margin-bottom: 0.3rem;
}

.response ul li {
    margin-bottom: 0.3rem;
}

/* Responsive layout */
@media (max-width: 768px) {
    .container {
        flex-direction: column;
    }
}
</style>

<div style="border: 1px solid #ccc; background-color: #f9f9f9; padding: 10px; border-radius: 5px; margin-bottom: 20px;">
  <strong>TL;DR:</strong> This post is mostly me following the steps of a Model Validation in Banking assisted by LLMs. The result is impressive to me in terms of quality and the time saving this could represent.
    At the end there is an app, front to back, for Model Validation if you want to see that.
</div>

* TOC
{:toc}

### Intro

Let's begin with what I mean by model validation in this post. Yes, it involves ensuring that models function correctly, a broad topic in itself, but this is specifically focused within the US banking environment. The rest of this section is just me asking ChatGPT about what a Model Validation in banking is, to both post it here for people that don't know about it and also to make sure understands how this works.

<div class="container">
<div class="text-column">
So, what does Chat GPT say about model validation in this context?
</div>
<div class="prompt-column">
<div class="prompt">
What does model validation mean in the US banking environment? Could you summarize it with few sentences?
<div class="response">
In the U.S. banking environment, model validation refers to the rigorous process of evaluating financial models to ensure they are reliable, accurate, and compliant with regulatory standards. It involves assessing the model's performance, assumptions, limitations, and potential risks. This practice is essential for managing model risk and adhering to regulatory guidelines such as SR 11-7, which requires banks to maintain robust model risk management frameworks to prevent financial and operational issues.
</div>
</div>
</div>
</div>
<div class="container">
<div class="text-column">
Alright, we have a base now. We understand that validating models is a regulated process guided by SR 11-7 so let's gather some more detail on what this should be:
</div>
<div class="prompt-column">
<div class="prompt">
What should a model validation contain according to SR 11-7?
<div class="response" markdown="1">
According to SR 11-7, a comprehensive model validation should include the following key components:
* **Evaluation of Conceptual Soundness:** An in-depth assessment of the model's design, theory, and logic, including underlying assumptions and methodology. This ensures that the model is appropriate for its intended purpose.
* **Ongoing Monitoring:** Regular checks to confirm that the model's performance aligns with expectations over time. This includes tracking key performance metrics and reviewing outcomes to identify any significant deviations.
* **Outcomes Analysis:** A quantitative comparison of the model's predictions against actual outcomes. This step helps assess the accuracy and reliability of the model in real-world conditions.
* **Benchmarking and Back-Testing:** Comparing the model's results with alternative models and validating performance through back-testing with historical data.
* **Assessment of Limitations and Uncertainty:** Identifying potential limitations in the model's framework and quantifying any associated uncertainties or risks.
These elements ensure a robust validation process, enabling institutions to identify and mitigate model risks effectively, maintaining compliance with regulatory requirements.
</div>
</div>
</div>
</div>

<div class="container">
<div class="text-column"> 
And what form and structure does a model validation take?
</div>
<div class="prompt-column">
<div class="prompt">
How should these results be presented?
<div class="response" markdown="1">
Model validation results, in line with SR 11-7, should be presented in a structured, transparent, and comprehensive report, typically covering the following sections:
* **Executive Summary**: A concise overview of the validation's key findings, model strengths and weaknesses, and an overall assessment of the model’s risk level and validation outcome (e.g., approved, approved with conditions, or not approved).
* **Scope and Objectives**: A clear outline of the validation’s purpose, including the model’s intended use, its assumptions, and limitations. It should specify the validation scope, such as whether it's a full-scope or targeted validation.
* **Methodology and Approach**: A description of the validation process, including the methods, tools, and benchmarks used. This part should clarify how the validation aligns with regulatory requirements and internal policies.
* **Evaluation of Conceptual Soundness**: Findings on the model’s design, methodology, assumptions, and overall soundness. This section often includes a detailed review of data sources, model components, and theoretical foundations.
* **Ongoing Monitoring and Performance Review**: Insights into the model’s stability, accuracy, and predictive power over time, along with performance metrics and any monitoring results that highlight trends or areas of concern.
* **Outcomes Analysis and Back-Testing Results**: Results of outcome analysis, including back-testing findings that compare predicted versus actual results. This section should provide quantitative and qualitative analyses, highlighting any discrepancies and their implications.
* **Issues and Recommendations**: Identification of any issues, weaknesses, or limitations, along with prioritized recommendations for improvements. This can include both short-term remediation actions and longer-term improvements to address identified risks.
* **Conclusion and Sign-Off**: A summary of key points and the final opinion on the model’s viability, along with sign-offs from validation team members and relevant stakeholders.
* **Supporting Documentation**: Additional materials such as data sources, code, validation testing details, and other relevant documents to ensure transparency and reproducibility.
Effective presentation should ensure clarity for stakeholders, including management and regulators, with visual aids like charts and tables where helpful.
</div>
</div>
</div>
</div>

Ok, so validating models in banking is assessing they work properly and then write the results in a report. And ChatGPT knows about it. 

While this process seems straightforward, it kind of is, it’s also incredibly useful. And it might be a topic for a different writing, but I don’t believe this practice is common outside of banking, which makes me think that other industries could benefit from adopting a similar approach: a standardized process that ensures models are reliable and used appropriately, with clear and consistent reporting.

Importantly, the goal of this process isn’t just to produce reports, it’s to improve the quality of models. And In my experience, it works. Development teams tend to build better products when they know their work will be audited, requiring them to justify decisions and document their processes in a way that business stakeholders can easily understand.

### A model

Let's now try to reproduce how a validation of a model would look like and how LLMs could assist with it. 

First, we need a model to validate. While almost any model could work for demonstration purposes, I wanted to choose something that resembles a real financial case. I searched on Kaggle and found this challenge:

> [Home Credit - Credit Risk Model Stability](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability). 
> 
> **Overview:** The goal of this competition is to predict which clients are more likely to default on their loans. The evaluation will favor solutions that are stable over time.

I am not sure how Kaggle works and it is sometimes confusing to me, but I found what appears to be an example notebook that implements a model for the problem, [Home Credit 2024 Starter Notebook](https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook/notebook). After downloading the data I have executed the code and it works. 

### Reviewing the code

Having a high level understanding of the implementation it is one of the first and fundamental steps in a model validation. LLMs might be able to help with this, and I will use Claude 3.5 Sonnet as seems to perform well with code related tasks, or at least is better than Chat GPT (which I found to be true after some tests where it proved to be significantly better).


<div class="container">
<div class="text-column"> 
Let’s see what the model can tell us about the code. I’ve converted the Jupyter notebook into a script, copied and pasted the resulting script into Claude AI’s prompt, and asked:
</div>
<div class="prompt-column">
<div class="prompt">
What is this code doing?
<div class="response" markdown="1">
Let me break down this code - it appears to be implementing a credit risk or default prediction model with a focus on stability across time. Here are the key components:

1 Data Loading and Preprocessing:
* Loads several CSV files containing different types of data (base tables, static data, credit bureau data, person data)
* Uses Polars (a fast DataFrame library) for initial data processing
* The data appears to be related to loan applications, with different tables containing various attributes about applicants

2 Feature Engineering:
* Creates aggregated features from person-related data:
  * Maximum occupation income
  * Whether any applicant is self-employed
  * Housing type for primary applicant
* Creates credit bureau features:
  * Maximum payments overdue
  * Whether payments were overdue by more than 31 days

3 Model Training:
* Uses LightGBM, a gradient boosting framework
* Splits data into train (60%), validation (20%), and test (20%) sets
* Trains a binary classification model with AUC (Area Under Curve) as the metric
* Uses early stopping to prevent overfitting

4 Model Evaluation:
 
* Calculates AUC scores for train, validation, and test sets
* Implements a custom stability metric called gini_stability that:
  * Tracks model performance (Gini coefficient) across different time periods (WEEK_NUM)
  * Penalizes: 
    * Declining performance over time (negative slope in performance)
    * High variance in performance (standard deviation of residuals)
  * This suggests the model is being used in a production setting where stable predictions over time are important

5 The hyperparameters used suggest a relatively conservative model:
* Max depth of 3
* Learning rate of 0.05
* Feature and bagging fractions < 1 for regularization

This code appears to be part of a production system where model stability over time is as important as raw predictive performance. The stability metric is particularly interesting as it penalizes both declining performance over time and high variance in performance, which are important considerations for deployed models.
</div>
</div>
</div>
</div>

This is already surprisingly good to me from a Validation point of view. It explains the code and also gets into some of the model parametrization and how that affects its behavior. And insights about the performance over time.

<div class="container">
<div class="text-column"> 
Let's ask for his opinion about the code:
</div>
<div class="prompt-column">
<div class="prompt">
Given this code, are there errors or would you recommend enhancements to it?
<div class="response" markdown="1">
Here are the key issues and recommended enhancements:
1. Type Conversion Issues:
``` Python
for df in [X_train, X_valid, X_test]:
    df = convert_strings(df)
```
This reassignment doesn't modify the original dataframes. Should be:
``` Python
X_train = convert_strings(X_train)
X_valid = convert_strings(X_valid)
X_test = convert_strings(X_test)
```

2.Missing Error Handling:
* Add error handling for file loading
* Validate data before model training
* Check for missing values

3.Performance Improvements:

* Consider using pl.LazyFrame for memory efficiency
* Move categorical conversion to set_table_dtypes function
* Use parallel processing for data aggregation
 
4.Model Enhancements:

* Add feature importance analysis
* Implement cross-validation
* Consider hyperparameter tuning
* Add model serialization

5.Output Enhancement:

* Add confidence intervals for predictions
* Include feature importance plots
* Add model performance metrics beyond AUC
</div>
</div>
</div>
</div>

While some of the recommendations lean more toward implementation design and might be more useful for developers, others provide good advice for validation purposes. For example, having evidence to support model parameterization is essential for a validation. Unlike Kaggle, where the primary objective is to achieve the highest score by any means, banking places a strong emphasis on justifying the decisions made in the model design.

### Validation tests

That said, with just a few minutes, I have good insights into this model’s implementation, and I think I could continue asking questions to gather more detailed and useful information for the validation. Now we can start implementing tests.

<div class="container">
<div class="text-column">
Let's ask the LLM for help.
</div>
<div class="prompt-column">
<div class="prompt">
Given this code to train a credit model, could you create and give me the tests that should be performed according to SR 11-7? The tests should have visualization when possible, the scope should be as broad as possible and the results compiled into a tex file, including the images. Could you make it in a way that all outputs (tex and images) are in the same folder?
<div class="response">
Lots of Python code.
</div>
</div>
</div>
</div>

<div class="container">
<div class="text-column">
The result from this request is some code that I execute in a cell right after the last one from the Notebook I downloaded. It works straight away, and after compiling the tex file, this is the result:
</div>
<div class="prompt-column">
<div class="prompt">
<div class="response" markdown="1">
![image](/assets/output1.jpg)
![image](/assets/output2.jpg)
![image](/assets/output3.jpg)
![image](/assets/output4.jpg)
![image](/assets/output5.jpg)
</div>
</div>
</div>
</div>

The result from this request is some code that I execute in a cell right after the last one from the Notebook I downloaded. It works straight away, and after compiling the tex file, this is the result:

![image](/assets/output1.jpg)
![image](/assets/output2.jpg)
![image](/assets/output3.jpg)
![image](/assets/output4.jpg)
![image](/assets/output5.jpg)

If the code and results are accurate, it's impressive. In just five minutes, I have a report that provides insights into fundamental aspects of the model.

While it is true that the report lacks detailed information, such as what the feature importance scores are, it also offers valuable data like feature correlations, model performance across datasets, and temporal performance trends

I'm sure some might find these tests simple and barely useful, but just taking a look at some of the submitted Kaggle solutions and my experience, I can tell that the needs are completely different to the credit models in finance I have been working with. 

For example, here it seems a matter of getting the maximum score in the competition, meaning that having an ensemble of complicated models with as many features as possible is the best way to go, my experience with credit models is that simplicity is highly valuable: there are compliance requirements and the users of the model that want to understand what the it is doing.

And the reason I am saying this is because the plot of the correlation matrix is straight away saying that the dark islands would require further assessment. This is a valid and actually a real test I would conduct to a model like this.

It is also interesting to see the PSI over time plot along with the average score trend. It seems that starting at week 60 something something is changing in the behavior of the model, and this is also something important to understand with credit models. It would depend on a lot of factors and this is a simplified example, but understanding what is going on with the model and what made its behavior change is important. 

In summary, these tests are good. And it took five minutes to "code", execute, and compile them into a PDF document. 
